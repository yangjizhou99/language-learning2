/**
 * Analyze unlabeled tokens - identify patterns and root causes
 * Run: npx tsx scripts/analyze-unlabeled-tokens.ts
 */

import { createClient } from '@supabase/supabase-js';
import * as dotenv from 'dotenv';
import * as path from 'path';

dotenv.config({ path: path.join(__dirname, '..', '.env.local') });

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!;
const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY!;
const supabase = createClient(supabaseUrl, supabaseKey);

interface UnlabeledToken {
    token: string;
    lemma: string;
    pos: string;
    type: 'grammar' | 'unknown';
    context?: string;
}

async function runAnalysis() {
    const { analyzeLexProfileAsync } = await import('../src/lib/recommendation/lexProfileAnalyzer');

    console.log('='.repeat(80));
    console.log('ğŸ” æœªæ ‡è®°ç­‰çº§Tokenåˆ†ææŠ¥å‘Š');
    console.log('='.repeat(80));
    console.log('');

    // Fetch test data
    const { data: items, error } = await supabase
        .from('shadowing_items')
        .select('id, text, title')
        .eq('lang', 'ja')
        .not('text', 'is', null)
        .neq('text', '')
        .limit(300);

    if (error || !items?.length) {
        console.error('Error:', error);
        return;
    }

    console.log(`åˆ†ææ ·æœ¬: ${items.length} ä¸ªæ—¥è¯­é¢˜ç›®\n`);

    const unlabeledGrammar: Map<string, { count: number; samples: string[]; pos: string; lemma: string }> = new Map();
    const unlabeledContent: Map<string, { count: number; samples: string[]; pos: string; lemma: string }> = new Map();

    console.log('æ­£åœ¨åˆ†æ...');

    for (let i = 0; i < items.length; i++) {
        process.stdout.write(`\r  è¿›åº¦: ${i + 1}/${items.length}`);

        try {
            const result = await analyzeLexProfileAsync(
                items[i].text,
                'ja',
                'kuromoji',
                'default',
                'hagoromo'
            );

            if (result?.details?.tokenList) {
                result.details.tokenList.forEach(t => {
                    // Grammar tokens without level
                    if (t.originalLevel === 'grammar') {
                        const existing = unlabeledGrammar.get(t.token);
                        if (existing) {
                            existing.count++;
                            if (existing.samples.length < 3 && !existing.samples.includes(items[i].text.slice(0, 50))) {
                                existing.samples.push(items[i].text.slice(0, 50) + '...');
                            }
                        } else {
                            unlabeledGrammar.set(t.token, {
                                count: 1,
                                samples: [items[i].text.slice(0, 50) + '...'],
                                pos: t.pos,
                                lemma: t.lemma
                            });
                        }
                    }

                    // Unknown content words
                    if (t.isContentWord && t.originalLevel === 'unknown') {
                        const existing = unlabeledContent.get(t.token);
                        if (existing) {
                            existing.count++;
                            if (existing.samples.length < 3 && !existing.samples.includes(items[i].text.slice(0, 50))) {
                                existing.samples.push(items[i].text.slice(0, 50) + '...');
                            }
                        } else {
                            unlabeledContent.set(t.token, {
                                count: 1,
                                samples: [items[i].text.slice(0, 50) + '...'],
                                pos: t.pos,
                                lemma: t.lemma
                            });
                        }
                    }
                });
            }
        } catch (e) {
            // Skip errors
        }
    }

    console.log('\n\n');

    // Sort by count
    const sortedGrammar = [...unlabeledGrammar.entries()].sort((a, b) => b[1].count - a[1].count);
    const sortedContent = [...unlabeledContent.entries()].sort((a, b) => b[1].count - a[1].count);

    // Analyze patterns
    console.log('='.repeat(80));
    console.log('ğŸ“ æœªæ ‡è®°è¯­æ³•è¯ (grammar æ— ç­‰çº§) - Top 50');
    console.log('='.repeat(80));
    console.log('');
    console.log('| Token | è¯æ ¹ | è¯æ€§ | å‡ºç°æ¬¡æ•° | å¯èƒ½åŸå›  |');
    console.log('|-------|------|------|----------|----------|');

    let grammarIssues = {
        singleChar: 0,
        verbEnding: 0,
        auxiliaryVerb: 0,
        particle: 0,
        other: 0
    };

    sortedGrammar.slice(0, 50).forEach(([token, info]) => {
        let reason = '';

        // Single character - likely grammar fragment
        if (token.length === 1) {
            reason = 'å•å­—ç¬¦(è¯­æ³•ç‰‡æ®µ)';
            grammarIssues.singleChar += info.count;
        }
        // Verb endings
        else if (/^(ãŸ|ã¦|ãªã„|ã‚Œã‚‹|ã›ã‚‹|ãŸã„|ã‚ˆã†|ã¾ã—|ã§ã™|ã¾ã™|ã‚ã‚‹|ã„ã‚‹|ãŠã‚‹|ãˆã‚‹|ã‚‰ã‚Œ|ã•ã›)$/.test(token)) {
            reason = 'åŠ¨è¯æ´»ç”¨è¯­å°¾';
            grammarIssues.verbEnding += info.count;
        }
        // Auxiliary verbs
        else if (info.pos.includes('åŠ©å‹•è©') || info.pos.includes('åŠ©è©')) {
            reason = 'åŠ©è¯/åŠ©åŠ¨è¯';
            grammarIssues.auxiliaryVerb += info.count;
        }
        // Particles
        else if (/^(ã‘ã©|ã‘ã‚Œã©|ã‹ã‚‰|ã¾ã§|ã‚ˆã‚Š|ã»ã©|ãªã©|ã¨ã‹|ã£ã¦|ãªã‚“ã¦|ã ã‘|ã—ã‹|ã°ã‹ã‚Š)$/.test(token)) {
            reason = 'æ¥ç»­åŠ©è¯';
            grammarIssues.particle += info.count;
        }
        else {
            reason = 'è¯åº“æœªæ”¶å½•?';
            grammarIssues.other += info.count;
        }

        console.log(`| ${token} | ${info.lemma !== token ? info.lemma : '-'} | ${info.pos} | ${info.count} | ${reason} |`);
    });

    console.log('');
    console.log('è¯­æ³•è¯é—®é¢˜åˆ†å¸ƒ:');
    console.log(`  - å•å­—ç¬¦ç‰‡æ®µ: ${grammarIssues.singleChar}`);
    console.log(`  - åŠ¨è¯æ´»ç”¨è¯­å°¾: ${grammarIssues.verbEnding}`);
    console.log(`  - åŠ©è¯/åŠ©åŠ¨è¯: ${grammarIssues.auxiliaryVerb}`);
    console.log(`  - æ¥ç»­åŠ©è¯: ${grammarIssues.particle}`);
    console.log(`  - å…¶ä»–(å¯èƒ½è¯åº“é—®é¢˜): ${grammarIssues.other}`);

    console.log('');
    console.log('='.repeat(80));
    console.log('ğŸ“š æœªæ ‡è®°å†…å®¹è¯ (unknown) - Top 50');
    console.log('='.repeat(80));
    console.log('');
    console.log('| Token | è¯æ ¹ | è¯æ€§ | å‡ºç°æ¬¡æ•° | å¯èƒ½åŸå›  |');
    console.log('|-------|------|------|----------|----------|');

    let contentIssues = {
        properNoun: 0,
        katakana: 0,
        compoundWord: 0,
        rareWord: 0,
        tokenizationError: 0
    };

    sortedContent.slice(0, 50).forEach(([token, info]) => {
        let reason = '';

        // Proper nouns (names, places)
        if (info.pos.includes('å›ºæœ‰åè©') || info.pos.includes('äººå') || info.pos.includes('åœ°å')) {
            reason = 'å›ºæœ‰åè¯';
            contentIssues.properNoun += info.count;
        }
        // Katakana words (likely loanwords)
        else if (/^[\u30A0-\u30FF]+$/.test(token)) {
            reason = 'ç‰‡å‡å(å¤–æ¥è¯­)';
            contentIssues.katakana += info.count;
        }
        // Long tokens (might be tokenization errors)
        else if (token.length > 6) {
            reason = 'é•¿è¯(å¯èƒ½åˆ‡åˆ†é—®é¢˜)';
            contentIssues.tokenizationError += info.count;
        }
        // Compound words
        else if (token.length >= 4 && /[\u4E00-\u9FFF]/.test(token)) {
            reason = 'å¤åˆè¯(è¯åº“æœªæ”¶å½•)';
            contentIssues.compoundWord += info.count;
        }
        else {
            reason = 'ä½é¢‘è¯(è¯åº“æœªæ”¶å½•)';
            contentIssues.rareWord += info.count;
        }

        console.log(`| ${token} | ${info.lemma !== token ? info.lemma : '-'} | ${info.pos} | ${info.count} | ${reason} |`);
    });

    console.log('');
    console.log('å†…å®¹è¯é—®é¢˜åˆ†å¸ƒ:');
    console.log(`  - å›ºæœ‰åè¯: ${contentIssues.properNoun}`);
    console.log(`  - ç‰‡å‡åå¤–æ¥è¯­: ${contentIssues.katakana}`);
    console.log(`  - é•¿è¯(åˆ‡åˆ†é—®é¢˜): ${contentIssues.tokenizationError}`);
    console.log(`  - å¤åˆè¯: ${contentIssues.compoundWord}`);
    console.log(`  - ä½é¢‘è¯: ${contentIssues.rareWord}`);

    console.log('');
    console.log('='.repeat(80));
    console.log('ğŸ“Š é—®é¢˜åˆ†ææ€»ç»“');
    console.log('='.repeat(80));
    console.log('');

    const totalGrammarUnlabeled = sortedGrammar.reduce((sum, [, info]) => sum + info.count, 0);
    const totalContentUnlabeled = sortedContent.reduce((sum, [, info]) => sum + info.count, 0);

    console.log(`æœªæ ‡è®°è¯­æ³•è¯æ€»æ•°: ${totalGrammarUnlabeled} (${sortedGrammar.length} ç§)`);
    console.log(`æœªæ ‡è®°å†…å®¹è¯æ€»æ•°: ${totalContentUnlabeled} (${sortedContent.length} ç§)`);
    console.log('');
    console.log('ä¸»è¦åŸå› åˆ†æ:');
    console.log('');
    console.log('ã€è¯­æ³•è¯ã€‘ä¸»è¦æ˜¯åˆ‡åˆ†äº§ç”Ÿçš„è¯­æ³•ç‰‡æ®µï¼Œå¦‚:');
    console.log('  - åŠ¨è¯æ´»ç”¨è¯­å°¾è¢«å•ç‹¬åˆ‡åˆ† (ãŸã€ã¦ã€ãªã„ã€ã‚Œã‚‹ç­‰)');
    console.log('  - åŠ©åŠ¨è¯æ´»ç”¨å½¢å¼æœªè¢«è¯­æ³•åº“è¦†ç›–');
    console.log('');
    console.log('ã€å†…å®¹è¯ã€‘ä¸»è¦æ˜¯è¯åº“è¦†ç›–é—®é¢˜:');
    console.log('  - å¤–æ¥è¯­(ç‰‡å‡åè¯)è¦†ç›–ä¸è¶³');
    console.log('  - å¤åˆè¯ã€æ´¾ç”Ÿè¯æœªæ”¶å½•');
    console.log('  - éƒ¨åˆ†å›ºæœ‰åè¯è¯¯åˆ¤ä¸ºå†…å®¹è¯');
    console.log('');
    console.log('='.repeat(80));
}

runAnalysis().catch(console.error);
