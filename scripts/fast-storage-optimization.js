#!/usr/bin/env node

/**
 * å¿«é€Ÿå¹¶å‘æ›´æ–° Supabase Storage æ–‡ä»¶çš„ç¼“å­˜å¤´
 * æ”¯æŒé«˜å¹¶å‘å¤„ç†ï¼Œå¤§å¹…æå‡å¤„ç†é€Ÿåº¦
 */

const { createClient } = require('@supabase/supabase-js');
require('dotenv').config({ path: '.env.local' });

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
const serviceKey = process.env.SUPABASE_SERVICE_ROLE_KEY;

if (!supabaseUrl || !serviceKey) {
  console.error('âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: NEXT_PUBLIC_SUPABASE_URL æˆ– SUPABASE_SERVICE_ROLE_KEY');
  process.exit(1);
}

const supabase = createClient(supabaseUrl, serviceKey);

// å¯é…ç½®çš„å¹¶å‘å‚æ•°
const CONFIG = {
  CONCURRENT_LIMIT: 20,        // åŒæ—¶å¤„ç†æ–‡ä»¶æ•°
  BATCH_DELAY: 100,            // æ‰¹æ¬¡é—´å»¶è¿Ÿ(ms)
  MAX_RETRIES: 3,              // æœ€å¤§é‡è¯•æ¬¡æ•°
  RETRY_DELAY: 1000,           // é‡è¯•å»¶è¿Ÿ(ms)
};

async function updateStorageCacheHeaders() {
  console.log('ğŸš€ å¼€å§‹å¿«é€Ÿå¹¶å‘æ›´æ–° Storage æ–‡ä»¶ç¼“å­˜å¤´...');
  console.log(`âš¡ å¹¶å‘è®¾ç½®: ${CONFIG.CONCURRENT_LIMIT} ä¸ªæ–‡ä»¶/æ‰¹æ¬¡`);
  
  const buckets = ['tts', 'recordings', 'audio'];
  const startTime = Date.now();
  
  for (const bucketName of buckets) {
    console.log(`\nğŸ“ å¤„ç†æ¡¶: ${bucketName}`);
    
    try {
      // é€’å½’è·å–æ‰€æœ‰æ–‡ä»¶
      const files = await getAllFilesRecursive(bucketName);
      
      if (files.length === 0) {
        console.log(`â„¹ï¸  æ¡¶ ${bucketName} ä¸ºç©ºæˆ–æ²¡æœ‰æ–‡ä»¶`);
        continue;
      }
      
      console.log(`ğŸ“Š æ‰¾åˆ° ${files.length} ä¸ªæ–‡ä»¶`);
      
      // å¿«é€Ÿå¹¶å‘å¤„ç†
      const { successCount, errorCount } = await processFilesConcurrently(bucketName, files);
      
      console.log(`\nğŸ“ˆ æ¡¶ ${bucketName} å¤„ç†å®Œæˆ:`);
      console.log(`   âœ… æˆåŠŸ: ${successCount}`);
      console.log(`   âŒ å¤±è´¥: ${errorCount}`);
      console.log(`   ğŸ“Š æˆåŠŸç‡: ${Math.round(successCount / (successCount + errorCount) * 100)}%`);
      
    } catch (error) {
      console.error(`âŒ å¤„ç†æ¡¶ ${bucketName} æ—¶å‡ºé”™:`, error.message);
    }
  }
  
  const totalTime = Math.round((Date.now() - startTime) / 1000);
  console.log(`\nğŸ‰ æ‰¹é‡æ›´æ–°å®Œæˆï¼æ€»è€—æ—¶: ${totalTime}ç§’`);
  console.log('\nğŸ’¡ å»ºè®®:');
  console.log('1. æ£€æŸ¥ Supabase Dashboard ä¸­çš„ Storage ä½¿ç”¨æƒ…å†µ');
  console.log('2. ç›‘æ§ Cached Egress æ˜¯å¦ä¸‹é™');
  console.log('3. è€ƒè™‘å°†çƒ­é—¨æ–‡ä»¶è¿ç§»åˆ°ç§æœ‰æ¡¶ + ç­¾åURL');
}

// é€’å½’è·å–æ‰€æœ‰æ–‡ä»¶
async function getAllFilesRecursive(bucketName, path = '') {
  const { data: items, error } = await supabase.storage
    .from(bucketName)
    .list(path, { limit: 1000 });
  
  if (error) {
    console.error(`âŒ åˆ—å‡ºè·¯å¾„ ${path} å¤±è´¥:`, error.message);
    return [];
  }
  
  if (!items || items.length === 0) {
    return [];
  }
  
  const files = [];
  for (const item of items) {
    const fullPath = path ? `${path}/${item.name}` : item.name;
    
    if (item.metadata && item.metadata.size) {
      files.push({ ...item, fullPath });
    } else {
      const subFiles = await getAllFilesRecursive(bucketName, fullPath);
      files.push(...subFiles);
    }
  }
  
  return files;
}

// å¹¶å‘å¤„ç†æ–‡ä»¶
async function processFilesConcurrently(bucketName, files) {
  let successCount = 0;
  let errorCount = 0;
  let processedCount = 0;
  
  // åˆ†æ‰¹å¤„ç†
  for (let i = 0; i < files.length; i += CONFIG.CONCURRENT_LIMIT) {
    const batch = files.slice(i, i + CONFIG.CONCURRENT_LIMIT);
    const batchNum = Math.floor(i / CONFIG.CONCURRENT_LIMIT) + 1;
    const totalBatches = Math.ceil(files.length / CONFIG.CONCURRENT_LIMIT);
    
    console.log(`ğŸ”„ å¤„ç†æ‰¹æ¬¡ ${batchNum}/${totalBatches} (${batch.length} ä¸ªæ–‡ä»¶)`);
    
    // å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
    const results = await Promise.allSettled(
      batch.map(file => processFile(bucketName, file))
    );
    
    // ç»Ÿè®¡ç»“æœ
    results.forEach((result, index) => {
      if (result.status === 'fulfilled' && result.value.success) {
        successCount++;
        console.log(`âœ… ${result.value.file}`);
      } else {
        errorCount++;
        const error = result.status === 'rejected' ? result.reason : result.value.error;
        console.error(`âŒ ${batch[index].fullPath}: ${error}`);
      }
    });
    
    processedCount += batch.length;
    const progress = Math.round(processedCount / files.length * 100);
    console.log(`ğŸ“Š è¿›åº¦: ${processedCount}/${files.length} (${progress}%)`);
    
    // æ‰¹æ¬¡é—´å»¶è¿Ÿ
    if (i + CONFIG.CONCURRENT_LIMIT < files.length) {
      await new Promise(resolve => setTimeout(resolve, CONFIG.BATCH_DELAY));
    }
  }
  
  return { successCount, errorCount };
}

// å¤„ç†å•ä¸ªæ–‡ä»¶
async function processFile(bucketName, file, retryCount = 0) {
  try {
    // ä¸‹è½½æ–‡ä»¶
    const { data: fileData, error: downloadError } = await supabase.storage
      .from(bucketName)
      .download(file.fullPath);
    
    if (downloadError) {
      throw new Error(`ä¸‹è½½å¤±è´¥: ${downloadError.message}`);
    }
    
    // é‡æ–°ä¸Šä¼ ï¼Œæ·»åŠ ç¼“å­˜å¤´
    const { error: uploadError } = await supabase.storage
      .from(bucketName)
      .upload(file.fullPath, fileData, {
        upsert: true,
        cacheControl: 'public, max-age=2592000, immutable',
        contentType: file.metadata?.mimetype || 'audio/mpeg'
      });
    
    if (uploadError) {
      throw new Error(`ä¸Šä¼ å¤±è´¥: ${uploadError.message}`);
    }
    
    return { success: true, file: file.fullPath };
    
  } catch (error) {
    // é‡è¯•é€»è¾‘
    if (retryCount < CONFIG.MAX_RETRIES) {
      console.log(`ğŸ”„ é‡è¯• ${file.fullPath} (${retryCount + 1}/${CONFIG.MAX_RETRIES})`);
      await new Promise(resolve => setTimeout(resolve, CONFIG.RETRY_DELAY));
      return processFile(bucketName, file, retryCount + 1);
    }
    
    return { success: false, error: error.message };
  }
}

// è¿è¡Œè„šæœ¬
updateStorageCacheHeaders().catch(console.error);
